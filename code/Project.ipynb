{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMszH-zAqBDj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFDZnwUqtgDg",
        "outputId": "6ae40e7a-e067-4eda-cd39-f6b39ebaf809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEwqLgHAuTa0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Make directory for Kaggle configuration\n",
        "os.makedirs('/root/.kaggle/', exist_ok=True)\n",
        "\n",
        "# Move the kaggle.json file to the correct location\n",
        "shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "\n",
        "# Set permissions for the kaggle.json file\n",
        "os.chmod('/root/.kaggle/kaggle.json', 600)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-oO2JazTrT"
      },
      "source": [
        "**Download Dataset to colab from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg0lgNRPucMN",
        "outputId": "3c820487-47c5-43dd-b5ab-ebfafb30497b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data\n",
            "License(s): other\n",
            "Downloading netflix-prize-data.zip to /content\n",
            " 97% 665M/683M [00:06<00:00, 118MB/s]\n",
            "100% 683M/683M [00:06<00:00, 115MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d netflix-inc/netflix-prize-data -p /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotDm8aBzdNM"
      },
      "source": [
        "**Extract the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S5oe5-YvPWp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/netflix-prize-data.zip'\n",
        "extract_path = '/content/netflix-prize-data'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyV0J4P-voVo",
        "outputId": "6fec0db9-a20c-46af-ad20-1e951120c890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.8.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFSYXerRzhbV"
      },
      "source": [
        "**Connect to mongodb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3BTX5DEvxZw"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# Replace with your MongoDB connection details\n",
        "client = MongoClient('mongodb://192.168.0.112:8081/')\n",
        "db = client['netflix']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8qfJCszznF8"
      },
      "source": [
        "**Store movie titles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbn3ZEW7yI5e"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "movies_collection = db['movies']\n",
        "\n",
        "batch_size = 10000  # Define your batch size\n",
        "batch = []\n",
        "\n",
        "with open('/content/netflix-prize-data/movie_titles.csv', 'r', encoding='ISO-8859-1') as file:\n",
        "    reader = csv.reader(file, delimiter=',')\n",
        "    for row in reader:\n",
        "        movie_id = int(row[0])\n",
        "        year = row[1]\n",
        "        title = row[2]\n",
        "\n",
        "        # Handle 'NULL' year\n",
        "        year = int(year) if year != 'NULL' else None\n",
        "\n",
        "        movie_doc = {\n",
        "            \"movie_id\": movie_id,\n",
        "            \"year_of_release\": year,\n",
        "            \"title\": title\n",
        "        }\n",
        "        batch.append(movie_doc)\n",
        "\n",
        "        if len(batch) >= batch_size:\n",
        "            movies_collection.insert_many(batch)\n",
        "            batch.clear()\n",
        "\n",
        "    # Insert remaining documents in the batch\n",
        "    if batch:\n",
        "        movies_collection.insert_many(batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGvNBkeHzxKF"
      },
      "source": [
        "**store ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1vdEnqw2NHP",
        "outputId": "4e5b0c75-03b1-4a7a-8e26-29fab60d838a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [1:51:34<00:00, 1673.53s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "ratings_collection = db['ratings']\n",
        "\n",
        "ratings_files = ['combined_data_1.txt', 'combined_data_2.txt', 'combined_data_3.txt', 'combined_data_4.txt']\n",
        "batch_size = 100000  # Define your batch size\n",
        "batch = []\n",
        "\n",
        "for filename in tqdm(ratings_files):\n",
        "    with open(os.path.join('/content/netflix-prize-data/', filename), 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        current_movie_id = None\n",
        "        for line in lines:\n",
        "            if line.endswith(':\\n'):  # Movie ID line\n",
        "                current_movie_id = int(line.strip().replace(':', ''))\n",
        "            else:\n",
        "                try:\n",
        "                    customer_id, rating, date = line.strip().split(',')\n",
        "                    rating_doc = {\n",
        "                        \"movie_id\": current_movie_id,\n",
        "                        \"customer_id\": int(customer_id),\n",
        "                        \"rating\": int(rating),\n",
        "                        \"date\": date\n",
        "                    }\n",
        "                    batch.append(rating_doc)\n",
        "                except ValueError as e:\n",
        "                    print(f\"Skipping line due to parsing error: {line} - Error: {e}\")\n",
        "\n",
        "                if len(batch) >= batch_size:\n",
        "                    ratings_collection.insert_many(batch)\n",
        "                    batch.clear()\n",
        "\n",
        "        # Insert remaining documents in the batch\n",
        "        if batch:\n",
        "            ratings_collection.insert_many(batch)\n",
        "            batch.clear()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Od2esyz42F"
      },
      "source": [
        "**store probe and qualifying data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4_5cBcDCWgh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "qualifying_collection = db['qualifying']\n",
        "probe_collection = db['probe']\n",
        "\n",
        "# Define file paths\n",
        "qualifying_file = 'qualifying.txt'\n",
        "probe_file = 'probe.txt'\n",
        "data_directory = '/content/netflix-prize-data/'\n",
        "\n",
        "# Function to process and insert data in batches\n",
        "def process_and_insert_data(filename, collection):\n",
        "    batch_size = 100000  # Adjust batch size based on your system and data size\n",
        "    batch = []\n",
        "    current_movie_id = None\n",
        "\n",
        "    with open(os.path.join(data_directory, filename), 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        current_movie_id = None\n",
        "        for line in lines:\n",
        "            if line.endswith(':\\n'):  # Movie ID line\n",
        "                current_movie_id = int(line.strip().replace(':', ''))\n",
        "            else:\n",
        "                try:\n",
        "                    if filename == qualifying_file:\n",
        "                        customer_id, date = line.strip().split(',')\n",
        "                        doc = {\n",
        "                            \"movie_id\": current_movie_id,\n",
        "                            \"customer_id\": int(customer_id),\n",
        "                            \"date\": date\n",
        "                        }\n",
        "                    elif filename == probe_file:\n",
        "                        customer_id = int(line.strip())\n",
        "                        doc = {\n",
        "                            \"movie_id\": current_movie_id,\n",
        "                            \"customer_id\": customer_id\n",
        "                        }\n",
        "                    batch.append(doc)\n",
        "\n",
        "                    # Insert batch if it reaches batch_size\n",
        "                    if len(batch) == batch_size:\n",
        "                        collection.insert_many(batch)\n",
        "                        batch = []\n",
        "\n",
        "                except ValueError as e:\n",
        "                    print(f\"Skipping line due to parsing error: {line} - Error: {e}\")\n",
        "\n",
        "        # Insert any remaining documents in the batch\n",
        "        if batch:\n",
        "            collection.insert_many(batch)\n",
        "\n",
        "# Process and insert qualifying data in batches\n",
        "process_and_insert_data(qualifying_file, qualifying_collection)\n",
        "\n",
        "# Process and insert probe data in batches\n",
        "process_and_insert_data(probe_file, probe_collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cjbdKq-hY4P9",
        "outputId": "73f06ef9-c0b3-4907-b5e7-035f64aee88f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'customer_id_1'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ratings_collection = db['ratings']\n",
        "ratings_collection.create_index([(\"customer_id\", pymongo.ASCENDING)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2njP4oK15tC"
      },
      "source": [
        "**Setup Spark environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03ulDtuEfbHM"
      },
      "outputs": [],
      "source": [
        "# scala_map = spark.conf._jconf.getAll()\n",
        "# python_dict = {}\n",
        "\n",
        "# iterator = scala_map.iterator()\n",
        "# while iterator.hasNext():\n",
        "#     entry = iterator.next()\n",
        "#     key = entry._1()  # Scala tuple's first element (key)\n",
        "#     value = entry._2()  # Scala tuple's second element (value)\n",
        "#     python_dict[key] = value\n",
        "\n",
        "# print(python_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSs8SQM2jqCt"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9GA1SwH9aOE",
        "outputId": "b4b7332f-e4f3-41d8-8b7b-25bd4ec5efac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.2.4\n",
            "  Downloading pyspark-3.2.4.tar.gz (281.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.2.4)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.4-py2.py3-none-any.whl size=282040918 sha256=88884de89b804803f7c6ca34ae306d8ac3ecd546774845f4d1012f19ec31f891\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/e3/c8/c358dac750f2b6a4b03328d10e05a5c69501664bd6504b6c3e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSSY1zwofth4",
        "outputId": "4edd507b-cb9c-477c-d61e-cfbaf9246923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Xc6f0CheB0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext, functions as F\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syMnyLrEjM-0"
      },
      "outputs": [],
      "source": [
        "!export JAVA_HOME=/usr\n",
        "!export PATH=$JAVA_HOME/bin:$PATH\n",
        "!export SPARK_HOME=/usr/local/lib/python3.10/dist-packages/pyspark\n",
        "!export PATH=$SPARK_HOME/bin:$PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovWVORkgct__"
      },
      "outputs": [],
      "source": [
        "# Configure Spark to use more memory\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"NetflixRecommendation\") \\\n",
        "    .set(\"spark.driver.memory\", \"4g\") \\\n",
        "    .set(\"spark.executor.memory\", \"4g\") \\\n",
        "    .set(\"spark.mongodb.read.connection.uri\", \"mongodb://192.168.0.112:8081/netflix\") \\\n",
        "    .set(\"spark.mongodb.write.connection.uri\", \"mongodb://192.168.0.112:8081/netflix\") \\\n",
        "    .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector:10.0.2\") \\\n",
        "    .set(\"spark.ui.port\", \"4050\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KdXlazAc6lA"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgFAjeUYfzOo",
        "outputId": "fc9612a9-9cce-4bfc-e280-6d3073a22462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark UI is available at: NgrokTunnel: \"https://a5a9-35-229-232-205.ngrok-free.app\" -> \"http://localhost:4050\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2ioatyYmk1ldEQPhNsFZlaRiOgb_7Wjbjommity4XFKATCUgX\")\n",
        "# Open a tunnel to the Spark UI port\n",
        "public_url = ngrok.connect(4050)\n",
        "print(f\"Spark UI is available at: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xou_HcHEdBV4"
      },
      "outputs": [],
      "source": [
        "ratings_df = spark.read.option(\"spark.mongodb.database\", \"netflix\") \\\n",
        ".option(\"spark.mongodb.collection\", \"ratings\") \\\n",
        ".option(\"partitioner\", \"com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner\") \\\n",
        ".option(\"partitioner.options.partition.size\",\"512\") \\\n",
        ".load(format='mongodb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7erZu85_53B",
        "outputId": "bc744aa3-c1c2-4d93-84b3-fd69e0889232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+----------+--------+------+\n",
            "|                 _id|customer_id|      date|movie_id|rating|\n",
            "+--------------------+-----------+----------+--------+------+\n",
            "|6685992708170e9aa...|    1488844|2005-09-06|       1|     3|\n",
            "|6685992708170e9aa...|     822109|2005-05-13|       1|     5|\n",
            "|6685992708170e9aa...|     885013|2005-10-19|       1|     4|\n",
            "|6685992708170e9aa...|      30878|2005-12-26|       1|     4|\n",
            "|6685992708170e9aa...|     823519|2004-05-03|       1|     3|\n",
            "|6685992708170e9aa...|     893988|2005-11-17|       1|     3|\n",
            "|6685992708170e9aa...|     124105|2004-08-05|       1|     4|\n",
            "|6685992708170e9aa...|    1248029|2004-04-22|       1|     3|\n",
            "|6685992708170e9aa...|    1842128|2004-05-09|       1|     4|\n",
            "|6685992708170e9aa...|    2238063|2005-05-11|       1|     3|\n",
            "|6685992708170e9aa...|    1503895|2005-05-19|       1|     4|\n",
            "|6685992708170e9aa...|    2207774|2005-06-06|       1|     5|\n",
            "|6685992708170e9aa...|    2590061|2004-08-12|       1|     3|\n",
            "|6685992708170e9aa...|       2442|2004-04-14|       1|     3|\n",
            "|6685992708170e9aa...|     543865|2004-05-28|       1|     4|\n",
            "|6685992708170e9aa...|    1209119|2004-03-23|       1|     4|\n",
            "|6685992708170e9aa...|     804919|2004-06-10|       1|     4|\n",
            "|6685992708170e9aa...|    1086807|2004-12-28|       1|     3|\n",
            "|6685992708170e9aa...|    1711859|2005-05-08|       1|     4|\n",
            "|6685992708170e9aa...|     372233|2005-11-23|       1|     5|\n",
            "+--------------------+-----------+----------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ratings_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVZyJQxldHne"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data if necessary\n",
        "ratings_df = ratings_df.withColumn(\"customer_id\", ratings_df[\"customer_id\"].cast(\"integer\"))\n",
        "ratings_df = ratings_df.withColumn(\"movie_id\", ratings_df[\"movie_id\"].cast(\"integer\"))\n",
        "ratings_df = ratings_df.withColumn(\"rating\", ratings_df[\"rating\"].cast(\"float\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOio2anSdS_9"
      },
      "outputs": [],
      "source": [
        "# Initialize ALS model\n",
        "from pyspark.ml.recommendation import ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW3LnwyddS80"
      },
      "outputs": [],
      "source": [
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.01,\n",
        "    userCol=\"customer_id\",\n",
        "    itemCol=\"movie_id\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xEStJvW4dS52"
      },
      "outputs": [],
      "source": [
        "# Train the ALS model on the entire dataset\n",
        "model = als.fit(ratings_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70orZj-sdSwo",
        "outputId": "12b28ca7-393f-4db0-8169-ba57fac8cf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Extract user and item factors\n",
        "user_factors = model.userFactors\n",
        "item_factors = model.itemFactors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qzIjcdAd97Z"
      },
      "outputs": [],
      "source": [
        "# Save user factors to MongoDB\n",
        "user_factors.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.database\", \"netflix\") \\\n",
        "    .option(\"spark.mongodb.collection\", \"user_factors\") \\\n",
        "    .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iKPgnIKd94n"
      },
      "outputs": [],
      "source": [
        "# Save item factors to MongoDB\n",
        "item_factors.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.database\", \"netflix\") \\\n",
        "    .option(\"spark.mongodb.collection\", \"item_factors\") \\\n",
        "    .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRjrY-R224AI",
        "outputId": "95fe009d-feb8-4755-e4cd-31596f70aa65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r_cupPUH27su"
      },
      "outputs": [],
      "source": [
        "# Path to save the ALS model in Google Drive\n",
        "model_path = '/content/drive/MyDrive/nrs1'\n",
        "\n",
        "# Save the ALS model using Spark's native format\n",
        "model.save(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cRB_3w3jGgR"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymuh0WwR7Gf9"
      },
      "source": [
        "**Check if saved model and loaded model are same**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hBhLqrn_5Q5L"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALSModel\n",
        "\n",
        "# Path to save the ALS model in Google Drive\n",
        "model_path = '/content/drive/MyDrive/nrs1'\n",
        "\n",
        "# Load the ALS model from the saved path in Google Drive\n",
        "loaded_model = ALSModel.load(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pNfJyiZm5l4L",
        "outputId": "444c90f3-f0ed-49ec-f363-037e1ffa69bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models have identical parameters.\n"
          ]
        }
      ],
      "source": [
        "# Compare model attributes\n",
        "if (loaded_model.userCol == model.userCol and\n",
        "    loaded_model.itemCol == model.itemCol and\n",
        "    loaded_model.rank == model.rank):\n",
        "    print(\"Models have identical parameters.\")\n",
        "else:\n",
        "    print(\"Models have different parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4bV-h0Ro06I"
      },
      "source": [
        "**Caching Data in Redis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuL3MmBtqvRX",
        "outputId": "1bef356f-2c74-4641-f16c-ef09b153657a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: redis in /usr/local/lib/python3.10/dist-packages (5.0.7)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.8.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install redis pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrrpYRJ6LcTn"
      },
      "outputs": [],
      "source": [
        "import redis\n",
        "import pymongo\n",
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNWUWJddlGc7"
      },
      "outputs": [],
      "source": [
        "# Connect to MongoDB\n",
        "mongo_uri = \"mongodb://192.168.0.112:8081/\"\n",
        "mongo_client = pymongo.MongoClient(mongo_uri)\n",
        "db = mongo_client[\"netflix\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAe71fb2lGOj"
      },
      "outputs": [],
      "source": [
        "# Connect to Redis\n",
        "redis_url = \"192.168.0.112\"\n",
        "redis_port = 8088\n",
        "redis_client = redis.StrictRedis(host=redis_url, port=redis_port, db=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgtW1uwvEw0o",
        "outputId": "471e8872-db16-44b7-acf1-1d80785e3d68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "redis_client.flushdb()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVs2LCLKxwuU"
      },
      "outputs": [],
      "source": [
        "# Function to fetch user factors from MongoDB and cache in Redis\n",
        "def cache_user_factors():\n",
        "    user_factors_collection = db['user_factors']\n",
        "    user_factors = list(user_factors_collection.find({}))  # Retrieve all user factors\n",
        "\n",
        "    # Use Redis pipeline for bulk update\n",
        "    pipeline = redis_client.pipeline()\n",
        "\n",
        "    # Iterate through each user factor and add to pipeline for bulk update\n",
        "    for user in user_factors:\n",
        "        user_id = str(user['id'])\n",
        "        user_factors_data = {\n",
        "            'features': json.dumps(user['features'])  # Ensure 'features' is serialized to JSON\n",
        "        }\n",
        "        pipeline.hset(f'user_factors:{user_id}', mapping=user_factors_data)\n",
        "\n",
        "    # Execute the pipeline to perform bulk update\n",
        "    pipeline.execute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw9iDCGax53n"
      },
      "outputs": [],
      "source": [
        "cache_user_factors()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmDH767oxzCy"
      },
      "outputs": [],
      "source": [
        "# Function to fetch item factors from MongoDB and cache in Redis\n",
        "def cache_item_factors():\n",
        "    item_factors_collection = db['item_factors']\n",
        "    item_factors = list(item_factors_collection.find({}))  # Retrieve all item factors\n",
        "\n",
        "    # Prepare item factors dictionary\n",
        "    item_factors_dict = {}\n",
        "    for item in item_factors:\n",
        "        item_id = str(item['id'])\n",
        "        item_factors_dict[item_id] = item['features']\n",
        "\n",
        "    # Store item factors dictionary as JSON in Redis\n",
        "    redis_client.set('item_factors', json.dumps(item_factors_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEcG8Wr2x75T"
      },
      "outputs": [],
      "source": [
        "cache_item_factors()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCuRLnUZx07f"
      },
      "outputs": [],
      "source": [
        "# Function to fetch movies from MongoDB and cache in Redis\n",
        "def cache_movies():\n",
        "    movies_collection = db['movies']\n",
        "    movies = list(movies_collection.find({}))  # Retrieve all movies\n",
        "\n",
        "    # Use Redis pipeline for bulk update\n",
        "    pipeline = redis_client.pipeline()\n",
        "\n",
        "    # Prepare movie data dictionary and add to pipeline for bulk update\n",
        "    for movie in movies:\n",
        "        movie_id = str(movie['movie_id'])\n",
        "\n",
        "        # Validate and convert title and year_of_release\n",
        "        title = movie.get('title', '')\n",
        "        year_of_release = movie.get('year_of_release', '')\n",
        "\n",
        "        # Ensure title and year_of_release are not None\n",
        "        if year_of_release is None:\n",
        "            year_of_release = \"None\"\n",
        "\n",
        "        movie_data = {\n",
        "            'title': title,\n",
        "            'year_of_release': year_of_release\n",
        "        }\n",
        "        pipeline.hset(f'movie:{movie_id}', mapping=movie_data)\n",
        "\n",
        "    # Execute the pipeline to perform bulk update\n",
        "    pipeline.execute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFUOt_UTx8wo"
      },
      "outputs": [],
      "source": [
        "cache_movies()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmX19pHZ8Efw"
      },
      "outputs": [],
      "source": [
        "# Function to cache user IDs in Redis\n",
        "def cache_user_ids():\n",
        "    user_factors_collection = db['user_factors']\n",
        "    user_factors = list(user_factors_collection.find({}))\n",
        "    # Extract unique user IDs\n",
        "    user_ids = [str(user['id']) for user in user_factors]\n",
        "\n",
        "    # Store user IDs in Redis set 'user_ids'\n",
        "    redis_client.sadd('user_ids', *user_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EUs3Dcm8z2g"
      },
      "outputs": [],
      "source": [
        "cache_user_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eftsd0Mwt7PN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fe0b17-6b8f-4650-c0a6-aee83855df7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting IMDbPY\n",
            "  Downloading IMDbPY-2022.7.9-py3-none-any.whl (1.2 kB)\n",
            "Collecting cinemagoer (from IMDbPY)\n",
            "  Downloading cinemagoer-2023.5.1-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (from cinemagoer->IMDbPY) (2.0.31)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from cinemagoer->IMDbPY) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->cinemagoer->IMDbPY) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->cinemagoer->IMDbPY) (3.0.3)\n",
            "Installing collected packages: cinemagoer, IMDbPY\n",
            "Successfully installed IMDbPY-2022.7.9 cinemagoer-2023.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install IMDbPY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJQal3Qdt44A"
      },
      "outputs": [],
      "source": [
        "# from imdb import IMDb\n",
        "\n",
        "# # Initialize IMDb instance\n",
        "# ia = IMDb()\n",
        "\n",
        "# # Function to fetch movie poster from IMDb\n",
        "# def fetch_movie_poster(title):\n",
        "#     search_results = ia.search_movie(title)\n",
        "#     if search_results:\n",
        "#         movie = search_results[0]\n",
        "#         ia.update(movie, info=['main'])\n",
        "#         if 'cover url' in movie:\n",
        "#             return movie['cover url']\n",
        "#     return None\n",
        "\n",
        "# # Function to fetch movies from MongoDB and cache in Redis with posters\n",
        "# def cache_movies_with_poster():\n",
        "#     movies_col = db['movies']\n",
        "#     movies_with_poster_col = db['movies_with_poster']\n",
        "\n",
        "#     # Use Redis pipeline for bulk update\n",
        "#     pipeline = redis_client.pipeline()\n",
        "\n",
        "#     # Fetch movies from MongoDB\n",
        "#     movies = list(movies_col.find({}))\n",
        "\n",
        "#     # Prepare movie data dictionary and add to pipeline for bulk update\n",
        "#     for movie in movies:\n",
        "#         movie_id = str(movie['movie_id'])\n",
        "\n",
        "#         # Validate and convert title and year_of_release\n",
        "#         title = movie.get('title', '')\n",
        "#         year_of_release = movie.get('year_of_release', '')\n",
        "\n",
        "#         # Fetch movie poster from IMDb\n",
        "#         poster_url = fetch_movie_poster(title)\n",
        "\n",
        "#         # Prepare data to store in MongoDB with poster info\n",
        "#         movie_data = {\n",
        "#             'movie_id': movie_id,\n",
        "#             'title': title,\n",
        "#             'year_of_release': year_of_release if year_of_release else \"None\",\n",
        "#             'poster_url': poster_url if poster_url else \"None\"\n",
        "#         }\n",
        "\n",
        "#         # Store in MongoDB collection with posters\n",
        "#         movies_with_poster_col.update_one({'movie_id': movie_id}, {'$set': movie_data}, upsert=True)\n",
        "\n",
        "#         # Update Redis cache with movie details\n",
        "#         pipeline.hset(f'movie:{movie_id}', mapping=movie_data)\n",
        "\n",
        "#     # Execute the pipeline to perform bulk update in Redis\n",
        "#     pipeline.execute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cache_movies_with_poster()"
      ],
      "metadata": {
        "id": "5-xUfXvTydIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}